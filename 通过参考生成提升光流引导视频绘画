```markdown
# RGVI (Reference-Guided Video Inpainting): Elevating Flow-Guided Video Inpainting with Reference Generation

## Research Background

Video inpainting: Temporal consistency, inpainting quality, and practicality are always the goals pursued in video inpainting. The goal of this paper is to overcome the limitations of optical flow-based methods, avoid artifacts caused by repeated sampling, and maintain sub-pixel accuracy. This paper creates a dataset-HQVI, which will not be explained here.

## RGVI Solution

### Basic Knowledge

Optical flow-based algorithms: Describe the direction and distance of pixel movement from one frame to another.

Pixel-wise optical flow tracking algorithm: For each missing pixel, track its corresponding pixel in adjacent frames based on the optical flow field.

Recursive pixel warping algorithm: Recursively apply the optical flow field for pixel warping (gradually warping pixels from the source frame to the target frame by applying the optical flow field multiple times).

Mesh deformation: Achieve pixel rearrangement by deforming the image grid, mainly used to map pixels from the source frame to the target frame.

### **Internal Pixel Propagation**

**One-shot pixel pulling method**

Purpose: Avoid error accumulation, improve inpainting accuracy, maintain sub-pixel accuracy, and reduce texture blurring.

Method: First, calculate the optical flow field between video frames. For each missing pixel in the target frame, track its corresponding pixel in the source frame based on the optical flow field (specifically, analyze the tracking method: for each missing element in the target frame, directly calculate its global corresponding position in the source frame through the chaining of the optical flow field), and pull the pixel color value from the source frame to the target frame in one shot (in contrast, the recursive pixel warping algorithm repeatedly samples, increasing errors). Finally, propagation verification is required, i.e., through a bidirectional pixel collection and verification protocol, detect unreliable propagation regions to ensure inpainting quality.

**Bidirectional Collection and Verification**

Purpose: Ensure the reliability of propagation, starting from the target frame, propagate forward and backward respectively to obtain global perception of the video.

Method: Prioritize collecting pixel color values from the nearest frames. Therefore, in each pass, we collect pixel color values for the missing regions in the target frame. Importantly, when looping through different source frames, we ensure that pixel color values are pulled in a one-shot manner, eliminating the need for repeated sampling. Once we find corresponding pixels in the forward and backward directions, we implement a verification protocol. Our method adopts a simple verification method based on color value differences. Specifically, we calculate the L1 distance between normalized 3-channel color values, ranging from 0 to 1. If the difference between the forward and backward pulled color values is below the threshold, we assign the average value to the target pixel position. Conversely, if there is a divergence (i.e., the difference exceeds the threshold), we mark these target pixels as unreliable and invalidate the propagation in subsequent steps. Note that the threshold is empirically set to 1, with minimal variation between different values.

### **Reference Generation and Propagation**

**Key Frame Selection**:

Purpose: To mitigate content conflicts between frames, we adopt a strategy where new content is generated only for a single key frame. Therefore, selecting key frames is crucial, and we need to quantitatively describe the influence of a frame.

Method: We determine key frames based on the influence after reference propagation (multiple key frames can also be determined as needed).

**Reference Generation**:

After selecting key frames, we use a large generative model (Stable Diffusion in this paper) to generate high-quality reference content. Two modes are proposed here: removal mode and generation mode.

In removal mode, the goal is to generate the most reasonable content that seamlessly blends with the original image (e.g., background). To avoid generating unwanted objects or patterns, we use "empty background, high resolution" as the text prompt for removal mode. In contrast, in generation mode, we use cropped images near the missing region as input images, thereby excluding external guidance from known pixels. This method aims to promote content generation based solely on local context.

**Reference Propagation**:

The generated pixels in the key frame are then propagated to the remaining frames using precomputed optical flow through mesh deformation operations.

### **Frame-by-Frame Completion**

Since some internally propagated pixels are marked as unreliable in the propagation verification step. These unreliable pixels cannot be adequately filled by inter-frame knowledge alone, so frame-by-frame completion is required.

Here, we adopt a lightweight convolutional network, structured as a simple encoder-decoder architecture, to process the missing regions of each frame separately.

### **Additional Masks for Handling Occlusions**

Purpose: Optical flow-based video inpainting methods are generally susceptible to occlusions. Ideally, optical flow should be completed consistently with the background content. However, the motion of occluding objects may significantly disrupt optical flow completion, leading to critical propagation errors.

Method: In addition to providing the mask of the target object to be removed (negative mask), also provide the mask of the object occluding the target (positive mask). Before inference, we combine these masks into a temporary target mask. After inference, we overlay the original content of the positive mask onto the output image, ensuring that only the target object is removed from the video.

**Negative Mask**: Represents the target object to be removed. In video inpainting tasks, the negative mask is used to mark the regions that need to be inpainted.

**Positive Mask**: Represents the object occluding the target. The positive mask is used to mark the regions occluding the target, ensuring that the content of the occluding object is retained during the inpainting process.

## Problems to be Solved

1. Advanced optical flow algorithms: Utilize deep learning for estimation.

2. Multi-frame optical flow.

3. Complex motion modeling.

4. Dynamic occlusion handling instead of positive masks.
```
