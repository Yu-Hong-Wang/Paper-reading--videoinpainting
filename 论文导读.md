# 《Towards Language-Driven Video Inpainting via Multimodal Large Language Models》

## 研究背景
传统视频修复依赖于手动标注的二进制掩码，希望通过自然语言来指导视频修复（之前图像修复的工作比较多）。随着MLLMS（多模态大语言模型）的发展，这一目标变得更加可行。

## 解决方法
### 构建ROVI数据集
这个数据集中包含三元组：原始视频、移除指令、修复结果（是用视频修复模型生成的，其视频质量存疑）。  
移除指令分成了两种，分别代表两种子任务：
- **参考视频修复**：简单的指令（一句话）指定修复对象。
- **交互式视频修复**：模拟对话的方式（更贴近现实，能实现更多细节）。

### LGVI和LGVI
基于Stable Diffusion框架，将网络架构进行了时间扩展，即在输入的维度上加入了时间维。并且引入时间注意力机制（关注时序信息，并且其复杂度低，不影响总的运算效率）。

LGVI的目标函数为 `l_diff + l_mask`。其中l_diff是扩散模型的损失，l_mask是掩码预测的损失。
LGVI-I with MLLM的目标函数为`l_diff + l_mask + l_lm`。其中l_lm是语言建模损失（即真实的句子分布和预测的句子分布之间的差异）
两个模型的主要区别在于原始模型直接将简短的文本编码后输入Cross Attention；而改进模型将MLLM的输出通过MM head输入Cross Attention。

## 待解决的问题
这篇文章主要提出了一个数据集和基线模型。并且引入了MLLMs引导修复。但是网络相比于其他方法而言效果不好，一致性存在严重问题，视频时长也比较短。
