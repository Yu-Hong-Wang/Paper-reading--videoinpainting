# 《AVID: Any-Length Video Inpainting with Diffusion Model》

## 研究背景

视频修复目前存在以下问题：时间一致性（物体在时间域上是连贯的）；不同类型修复任务（替换物体，修改颜色纹路，视频延长等所需要的结构保真度不同）；视频长度要求（需要尽可能地能够处理更长的视频）。

## 解决方法--AVID框架

### 运动模块（Motion Modules）

目的：确保时间一致性。

方法：给定一个视频和其第一帧的掩码区域，首先通过XMem跟踪整个视频的掩码区域。然后遵循AnimateDiff的方法将图像扩散模型的2D层扩展为伪3D层，并添加额外的运动模块来学习帧之间的时间相关性。由文章中的模型架构图可以看出，在训练时T2I模型的权重是冻住的，只训练运动模块（伪3D卷积层和时间自注意力机制）。

### 结构引导模块（Structure Guidance Module）

目的：针对不同修复任务。

方法：训练结构条件模块，记为$s_{\theta}$（需要被优化的参数）。我们首先使用结构信息提取器**S**来获取每一帧的结构条件，然后通过结构引导模块输出$c_{s}$（由13个特征图组成）。随后这些特征图被集成到$ϵ_{\theta}$的跳跃链接和中间块中（具体详见UNet架构）。

**注意**：本文提出了结构引导的缩放因子$w_{s}$，但是这个参数是固定的。能不能设置为可学习的参数？

### 零样本推理管道（Zero-shot Inference Pipeline）

目的：处理任意长度的视频。
由**时间多扩散采样**和**中帧注意力引导机制**组成。

时间多扩散采样（Temporal MultiDiffusion Sampling）:首先将视频分割为多个重叠的短片段，然后对于每个短片段模型都会生成一个修复结果，对于同一帧会同时出现在第一个片段和第二个片段的情况，我们需要将重复片段进行平均，即得到最终结果。

中帧注意力引导机制（Middle-frame Attention Guidance）:在修复视频时会出现随着时间，某个物体的形状或颜色等发生慢慢变化。我们选择中间帧作为参考帧，确保视频整体的一致性。具体操作为：在生成每一帧时，模型不仅仅会考虑当前帧的信息还会参考中间帧的信息（具体公式参考论文）：

Attention(ψ^i) = softmax((Q^i K^{iT}) / sqrt(d)) V^i * (1 - ω) + softmax((Q^i K^{⌈N'/2⌉T}) / sqrt(d)) V^{⌈N'/2⌉} * ω

## 待解决的问题

1.网络架构和运动模块有改进空间。

2.能不能更好地提取结构信息以及如何融合进目标函数。

3.模型风格迁移与融合

4.视频修复可控性

5.探索基于物理的模型，这样才能完全真实！
